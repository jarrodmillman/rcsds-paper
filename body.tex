\section{Introduction}

%traditional approach vs. our approach to teaching neuroimaging analysis;
%bottom up (low-level) vs. top down (high-level);
%project-based;
%interdisciplinary

We begin by explaining the background and rationale (\S~\ref{background}) of our approach to the course.
In material and methods (\S~\ref{methods}), we describe the course format, student audience,
and curriculum.
Student projects are presented in results (\S~\ref{results}).
Finally, we discuss (\S~\ref{discussion}) adapting the course to
different student audiences.

\section{Background and Rationale}\label{background}

Between us, we have many decades experience of teaching neuroimaging analysis.
% Matthew 1997- = 20
% JB 1997ish- 20
Like most other teachers of imaging, we have taught traditional courses with a
mixture of lectures covering the broad general ideas of the analysis, usually
combined with some practical workshops using imaging software.

We also have a great deal of experience of giving practical support for
imaging analysis to graduate students and post docs.

Our experience has gradually brought us to realize that we have three major
problems in the form of traditional teaching.

The first is that our students often leave their education with a superficial
understanding of imaging methods.  They are familiar with the steps, and the
terms, but when we ask them to reason about their analysis, they struggle.

The second problem is that the idealized picture in the lectures bears a
slight resemblance to their everyday work, which involves: using a Unix
computer, often remotely; collating and manipulating data; writing and sharing
scripts in computing languages such as Matlab or the Unix shell-scripting
languages.  Analyses can and do become messy and complicated, leading to
confusion and error.  Most labs expect students to learn how to do this on the
job, and few labs use current computing tools and process to control 
complexity and error.  As a corollary, it is usually difficult for a
researcher to share their analysis with another researcher, even in their own
lab, and it is hard for the PI to check the analysis for errors or invalid
assumptions.

The final problem is that we have made it hard for imagers to collaborate with
other technical fields such as engineering and statistics.  Imaging software
packages are highly specific to brain imaging, and present an interface to
common statistical procedures that our students do not recognize.  Many
students do not have much background in standard engineering tools such as
convolution, the Fourier transform or numerical optimization.  In traditional
teaching, we usually gloss over the details of these fields, which saves us
time, but leaves the students without the understanding or vocabulary to
explain their analysis problem to other scientists in a way that makes it easy
for them to engage.  The result may be that imaging research is more likely to
live in a neuroscience ghetto that is effectively walled off from
collaboration and insight by other scientists.

After years of trying to patch up the problems caused by our own teaching, we
came to the conclusion that the traditional approach is based on a fundamental
error of emphasis.

Functional imaging analysis touches on many theoretical and practical fields,
including signal and image processing, linear algebra, statistics, computing,
coding and data management.  Teaching all these topics in depth would take
longer than most students or institutions have time for.  We need to choose
what we will teach, and what we will leave for the students to pick up later.
We have to decide what is fundamental, and what is peripheral.

The traditional approach takes the top level to be fundamental.  It aims to
give the big picture, leaving the students to fill in the details. The
assumption is that the student will either know or pick up some of the
mathematical and practical foundations later, with the big picture as a
framework to guide them in choosing what to learn.

Our experience is that this assumption is quite incorrect.  It is rare for
students to go on to learn these foundations, and common for researchers to
continue to work with a limited and superficial understanding.  We believe
this has severe consequences for the quality of their work; because we did not
teach them these foundations, they come to believe that they do not need to
know them.  The foundations appear to them to be the preserve of clever
mathematical and computing types; they have no hope of understanding them in
any depth.  They come to treat the analysis as something special and magic,
where the rules are determined by practice, because they cannot hope to reason
about them.

Here we propose an alternative to the traditional approach.  Instead of
concentrating on the big picture, we emphasize the foundations on which the
analysis is built, and their implementation in practice, with code and
computing.  We were inspired by the famous epithet of Richard Feynman, found
written on his blackboard after his death: ``What I cannot create, I do not
understand.''\footnote{\url{http://archives.caltech.edu/pictures/1.10-29.jpg}}
We aim to give the students a sufficient grasp of how the methods work, that
they could implement their own crude versions of the major algorithms.  In
teaching them to work with code, we also teach them standard programming
techniques for reducing complexity and controlling error.  We encourage them
to own their own learning, and explore their data, by devoting a large
proportion of the course and course marks to an open-ended group project that
must be public and completely replicable, down to the figures used in the
final report.

We will concentrate on three examples to illustrate the contrast between the
traditional approach and our own.

\subsection{The General Linear Model}

We have been thinking about how to teach the General Linear Model (GLM) for
many years
\citep{poline2012general}.\footnote{\url{http://imaging.mrc-cbu.cam.ac.uk/imaging/PrinciplesStatistics}}\footnote{\label{glm_intro}
\url{http://matthew-brett.github.io/teaching/glm_intro.html}}
\fixme{for statistician's we should comment that this is just the linear regression model,
or linear model.}

The GLM is a matrix formulation of multiple regression that can implement
familiar analysis methods such as Analysis of Variance (ANOVA).  Many students
have not taken linear algebra, or if they have, do not remember it very well.
The traditional teaching approach is to show how the GLM relates to simple and
multiple regression over a few slides in an hour-long talk, and then move on
to more advanced topics such as temporal filtering or random effects.
Although the students may have the impression that they have understood, we
have found that most of them are unable to answer simple questions about the
GLM formulation after such a talk, and that they rarely go on to improve their
understanding in their later education.

Our current teaching works through the GLM very slowly. We found that we
needed to do this for the students to see that the GLM is just another way of
expressing statistical methods that they already know, such as regression,
$t$-tests and ANOVA.  We start with simple regression, showing how the
regression can be expressed as the addition of vectors and vector / scalar
multiplication.  This leads to the matrix formulation of simple regression,
and thence to multiple regression.  We introduce dummy indicator variables to
express group membership and show how these relate to group means.  We do all
this using simple Python code to show them how the mathematics works in
practice, and give them sample code that they can edit and run for themselves.\cref{glm_intro}

In contrast, we rely on the students to work through more advanced concepts
such as temporal filtering and random effects.

\subsection{Project-based}

\begin{quote}\emph{
Computational reproducibility \citep{buckheit1995wavelab} and collaboration are
too abstract in the absence of a significant concrete project.  If you have a
tiny dataset and a small handful of tiny functions, you can just throw them all
in a directory and post them online.  Problems compound as data grows, analysis
complexity increases, and lines of code multiply.
}\end{quote}


\subsection{Programming}

Traditional courses do not use code for teaching; the hope is that the student
will pick up the code that they need while they are working, or by taking an
unrelated general course on programming.  Many lab heads without programming
expertise argue that teaching students to use code is not necessary, because
scientists do not need to be programmers.  Unfortunately, imaging researchers
do need to use, write and share code.  If we do not teach them to do this
correctly, our experience suggests that scientists tend to write sloppy and
poorly-structured code, with many errors.  Because they have no training in
correct practice, they do not learn, and the quality of their code does not
improve with experience.  In practice, imaging researchers {\em are}
programmers; but if we do not train them, they will be incompetent
programmers, and they will likely continue to be incompetent programmers for
the rest of their careers.

For this reason, we introduced standard best coding practice from the
beginning of our course, including testing, code review, measuring test
coverage, version control, and continuous automated testing with different
software versions. All final project work used public online repositories for
version control, code review, automated code coverage and testing.  The
projects gave the students an experience of best practice in collaboration
with code and data that they could apply to their future work.


\section{Material and Methods}\label{methods}

During the fall semester of 2015, we co-taught \emph{Reproducible and Collaborative
Statistical Data Science},\footnote{\url{http://www.jarrodmillman.com/stat159-fall2015/}}
a new project-based course offered through the Department of Statistics at UC Berkeley.
The course was open to upper-level undergraduates (as STAT 159) as well as
first-year graduate students (as STAT 259).
Students were required to have a basic undergraduate level familiarity with
probability, statistics, and statistical computing in R.
Many students were from statistics and/or computer science; however, we also
had students from many disciplines including cognitive science, psychology, and
architecture.
During the 15-week long semester, students were provided three hours of class
per week in two 90-minute sessions, plus two hours of labwork.
Students were expected to work at least eight hours per week outside class.
Project reports were due two weeks after the last class.

\subsection{Course overview}

\begin{quote}
A project-based introduction to statistical data analysis. Through case
studies, computer laboratories, and a term project, students learn
practical techniques and tools for producing statistically sound and
appropriate, reproducible, and verifiable computational answers to
scientific questions. Course emphasizes version control, testing,
process automation, code review, and collaborative programming.
Software tools include Bash, Git, Python, and \LaTeX.

\hfill\emph{---from the course catalog description}
\end{quote}

\emph{Reproducible and Collaborative Statistical Data Science} is a
project-based course that introduces students to reproducible and collaborative
statistical research, applied to a real scientific question.
Our objective for the course was for students to
(a) be proficient at the Unix commandline,
(b) be expert at version control with Git,
(c) be able to write documents in Markdown and \LaTeX,
(d) be familiar with scientific computing in Python,
(e) understand the computational and statistical issues involved with reproducibility, and
(f) be familiar with computational issues in modern statistical data
analysis through hands-on analysis of neuroimaging data.


\blockpar{Neuroimaging data analysis.}
In Section~\ref{project} we discuss the course project and
in Section~\ref{analysis} we outline how we taught the
students about neuroimaging data analysis.
First we describe the software stack and the class structure.

\blockpar{The software stack.}
Python for neuroimaging analysis \citep{millman2007analysis} and
software development best practices \citep{millman2014developing}.

\begin{itemize}
\item \textbf{The Unix environment.}
Introduction to the command line interface, the filesystem hierarchy, and
working with text files
\citep{preeyanon2014reproducible}.\footnote{\url{http://www.jarrodmillman.com/rcsds/standard/bash.html}}

\item \textbf{Version control with Git.}
Introduce version control with
Git\footnote{\url{http://www.jarrodmillman.com/rcsds/standard/git-intro.html}}
focusing on conceptual model and basic objects.

\item \textbf{Process automation with Make.}

\item \textbf{Text processing with \LaTeX and pandoc.}

\item \textbf{Scientific computing with Python.}
Introduction to the scientific Python software stack
\citep{millman2011python, perez2011python};
Introduction to Python;\footnote{\url{http://www.jarrodmillman.com/rcsds/standard/python.html}}
Basic NumPy, SciPy, and matplotlib;\footnote{\url{http://www.scipy-lectures.org/}}

\item \textbf{Continuous integration with GitHub, Travis CI, and Coveralls.io.}

\end{itemize}

\begin{quote}\emph{
As if the teams were working in a scientific research group or in industry, they were
given a set of tools (a ``software stack'') and practices, which they were required to
master and use. As if you were working in a scientific research group or in
industry, you will be pointed to resources to help you learn the tools, and it
is your responsibility to ensure that you have mastered them, which will
require a substantial amount of time outside class. And, as if you were working
in a scientific research group or in industry, you will be required to
collaborate and to take responsibility for your role in the collaboration.
}

\emph{
But the point is not merely to master the tools or to learn to collaborate
effectively.  Rather, the point is to do sound computational science, and to do
it in a way that others can verify the data and statistical techniques behind
your analysis, can verify that your code does what it's supposed to do, can run
the code using the data and parameters you used, can verify that they get the
same results you do, and can build on what you have done to advance science
even further.
}\end{quote}

\blockpar{Lectures, labs, homeworks, readings, and quizzes.}

\begin{quote}\emph{
The weekly lectures and labs prepared students---through a
series of guided exercises---with the basic skills and
background needed for the group project.
While we demonstrated basic methods and tools during lecture and lab,
students were expected to do additional research and reading in the course
of working on the group project.
For instance, they may have needed to use a specialized analysis
method or a Python package not covered in the lectures or labs.
}\end{quote}

\begin{quote}\emph{
learn what we need to know, when we need to know it; just-in-time learning 
}\end{quote}

homeworks

readings, and quizzes


\subsection{Course project}\label{project}

Our semester project involved investigating a published result using the
analysis of neuroimaging data.\footnote{There was no requirement
for students to have a background in neuroscience.  The intention of focusing
on neuroimaging data was  merely to provide a concrete problem domain that
exemplifies the types of programming and statistical challenges present in many
modern statistical applications.}
Groups of three to five students formed teams.
Each team chose a published neuroimaging study to investigate and
decided what they meant by \emph{investigate} in consultation with us.

\begin{quote}\emph{
(55\% of grade)
Students gain experience acquiring, cleaning, and curating data; formulating
scientific questions statistically; developing appropriate statistical methods
to analyze the data to answer the scientific questions; implementing those
methods in robust, testable, reusable, extensible software; applying the
methods; visualizing the results; interpreting the results; and communicating
the results to others. The students learn to do this in a way that is
computationally reproducible.
}\end{quote}

\begin{quote}
\begin{flushleft}
Form teams \dotfill Week 5\\
Project proposal \dotfill Week 6\\
Progress presentation \& draft report \dotfill Week 12\\
Project presentation \dotfill Week 15\\
Final report \dotfill Week 17\\
\end{flushleft}
\end{quote}

\blockpar{Repository.}
All project artifacts were stored in a repository.
See Figure~\ref{fig:repo} for a skeleton project repository listing.

\begin{quote}\emph{
The most important piece of documentation in your entire project is the README.
It is your responsibility to tell new users EXACTLY how to install, configure,
and run all aspects of your project. How you have the individual tasks broken
down is up to you, but you must very clearly state how to operate the
components of your project and in what order to do so. For example, one section
of your README (preferably the first section) might give us an enumerated list
of task to carry out, like:
}\end{quote}

\begin{verbatim}
    make data - downloads the data 
    make validate - ensure the data is not corrupted
    make eda - generate figures from exploratory analysis
    make analysis - generate figures and results
    make report - build final report
\end{verbatim}

\begin{quote}\emph{
Again, how you structure things is up to you, but it is very important that you
notify us of this structure. If the instructions aren't clear and we just have
to "figure it out", you're relying on us going through the same progression as
you by chance: this is a bad bet. 
}

\emph{
The README should also contain other pertinent information: for example, if
your "make data" step is going to download 5 Gb of data, it is very prudent to
warn users of this so they make sure they have enough disk space and are on a
stable internet connection before trying to do so.
}

\emph{
As part of your final project grade, you will be required to work on your
project using GitHub's pull request and code review mechanism.  During lecture,
I will cover the exact workflow you will be expected to follow.  Please note
that as this course is explicitly about reproducibility and collaboration your
project grade will not be entirely based on your final report, but will also
reflect how well your group work is reproducible and how effectively you
collaborated using the techniques taught in the course (e.g., pull requests,
code review, testing, etc.)
}\end{quote}

 

\blockpar{Data.}
\begin{quote}
OpenfMRI\footnote{\url{https://www.openfmri.org/}}---resource for publicly-available fMRI datasets
\citep{poldrack2013toward,poldrack2015openfmri}.
\end{quote}

\blockpar{Proposal.}

\begin{quote}\emph{
Identify a published fMRI paper and the accompanying data.
You should explain the basic idea of the paper in a paragraph.
You should also perform basic sanity check on the data
(e.g., can you downloaded, can you load the files, confirm that you have the
correct number of subjects).
}\end{quote}

\begin{quote}\emph{
Briefly explain what approach you intend to take for exploring
the data and paper.  If you intend to ``reproduce'' some aspect of the paper,
explain in exactly what sense you mean to do this.  If you are thinking about
validating the data describe the assumptions used in the analysis and indicate
how you might check them.
}\end{quote}

\begin{quote}\emph{
%we should use actual examples here or remove it
For example, for one team this might mean a very careful reanalysis of the data
using the original methods as closely as possible.  However, this shouldn't
mean merely running existing scripts used in the original analysis; rather, the
team would need to reimplement the analysis scripts.  Another team might decide
to conduct a different analysis than used in the original study.  For instance,
the published work might use a parametric approach and the team project might
attempt to use a nonparametric technique such as permutation testing.  A third
team might focus on a careful validation of the modeling assumptions made by
the original analysis.
}\end{quote}

\blockpar{Code.}

\blockpar{Slides.}

\blockpar{Report.}


\subsection{Neuroimaging data analysis}\label{analysis}

\begin{quote}\emph{
Our aim is for students to
(a) understand the basic concepts in neuroimaging,
and how they relate to the wider world of statistics, engineering, computer science;
(b) be comfortable working with neuroimaging data and code,
so they can write their own basic algorithms, and understand other people's code;
(c) work with code and data in a way that will save them time, help them collaborate,
and continue learning.%this isn't quite parallel
}\end{quote}


convolution (hemodynamic modeling, smoothing);

interpolation (slice time correction, image resampling);

optimization (registration, advanced statistics);

basic linear algebra (statistics).



\section{Results}\label{results}

12 teams of 3 to 5 students

Grading included checking that we could reproduce everyone's work, reviewing pull
requests and code contributions, checking that all code included
(passing) tests, reviewing code to see if it was well written, and
reading the final reports among other things. 
See Table~\ref{tab:rubric} for the final project grading rubric.

%\url{https://raw.githubusercontent.com/jarrodmillman/rcsds/master/notes/rubric.rst}
%\url{http://www.jarrodmillman.com/rcsds/notes/rubric.pdf}

discuss general quality

discuss a couple of example projects
\citep{tom2007neural}

\section{Discussion}\label{discussion}

How would this work for Psychology students?  Psych 214 follow-up

How would this work in a quarter system?

lessons learned ...


\section*{Conflict of Interest Statement}

The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.

\section*{Author Contributions}

KJM was the lead instructor and was responsible for the syllabus and project timeline;
KJM and MB were responsible for lectures and created homework assignments;
KJM and RB were responsible for labs, readings, quizzes, and grading;
all authors held weekly office hours to assist students with their projects.
KJM and MB wrote the first draft of the manuscript;
all authors wrote sections of the manuscript, contributed to manuscript revision, 
as well as read and approved the submitted version.
