\section{Introduction}

Few neuroimaging analyses are computationally reproducible,\footnote{Following
\citet{buckheit1995wavelab}, we define an analysis as \emph{computationally
reproducible} when someone other than the original author of an analysis can
produce on their own computer the reported results using the author's data,
code, and instructions.}
even between researchers in a single lab.
The process of analysis is typically ad-hoc and informal; the final analysis
is often the fruit of a considerable amount of trial and error, based in part
on scripts and data structures that the author inherited from other members of
the lab.
This process is
(a) confusing, leading to unclear hypotheses and conclusions,
(b) error prone, leading to incorrect conclusions and greater confusion,
and (c) an impractical foundation on which to build reproducible analyses.

The trifecta of confusion, error, and lack of reproducibility are ubiquitous
problems of computation that programmers have been fighting since before ``programmer''
became a job title. There are widely accepted tools and processes to
control this trifecta. These tools include the Unix command line, version
control, high-level readable programming languages, code review, breaking code
into manageable functions and objects, writing tests for all code,  and
continuous automatic test execution.

Most researchers accept that learning these techniques is desirable, but
object that a neuroimager has a limited amount of time they can spend in
class.  They may argue that this time should be spent learning an overview of
neuroimaging analysis, rather than tools and process for efficient
development.

In the course we describe here, we tested our hypothesis that we could
effectively teach \emph{both} the tools for efficient and reproducible work \emph{and}
the principles of neuroimaging, by building the course around a substantial
collaborative project, and putting the tools into immediate practice.

At intake, our students had little or no prior exposure to neuroimaging,
or to the computational tools and process we listed above.  We set them the
open-ended task of designing and executing a project, which was either a
replication or an extension of a published neuroimaging analysis, built from
code they had written themselves.  We required the analysis to be computationally
reproducible; the graders had to be able to figure out from a short text file
how to use a simple set of commands to
fetch the data, run the analysis, and generate the final report, including the
figures.  As we report in the results, our students were remarkably
successful in achieving this goal.  We argue that this approach to
training in neuroimaging is likely to be far more fruitful than traditional
imaging courses in preparing our students to do efficient, valid, and
reproducible work.

%We begin by explaining the background and rationale (\S~\ref{background}) of
%our approach to teaching students how to analyze neuroimaging data.
%In material and methods (\S~\ref{methods}), we describe the course format, student audience,
%and curriculum.
%Student projects are presented in results (\S~\ref{results}).
%Finally, we discuss (\S~\ref{discussion}) adapting the course to
%different student audiences, ...

\section{Background and Rationale}\label{background}

Between us, we have many decades experience of teaching neuroimaging analysis.
Like most other teachers of imaging, we have taught traditional courses with a
mixture of lectures covering the broad general ideas of the analysis, usually
combined with some practical workshops using one of the standard imaging
software packages.

We also have a great deal of experience of giving practical support for
imaging analysis to graduate students and post docs.

Our experience has gradually brought us to realize that this form of teaching
does a poor job of preparing our students for a productive career in imaging
research, in two fundamental ways:

\begin{itemize}

\item
    Computation with large complex datasets is difficult and distracting;
        without training in standard practice to reduce this distraction, we
        condemn our students to a life-time of inefficient work, and slow
        learning.

\item
    Standard teaching assumes either that the students already understand
        linear algebra, or that they do not need to understand it.  In our
        experience, both of these assumptions are mostly false, with the
        result that the students do not have the foundation on which they can
        build further understanding.

\end{itemize}

As a corollary of these two problems, imaging researchers usually do not have
the vocabulary, understanding or shared tools that make it possible to
collaborate fluently with researchers from other fields, such as statistics or
engineering.

We believe these two problems stem from a fundamental error of emphasis.
The traditional approach to teaching takes the top level to be fundamental.
It aims to give the big picture, leaving the students to fill in the details.
The assumption is that the student will either know or pick up some of the
mathematical and practical foundations later, with the big picture as a
framework to guide them in choosing what to learn.

In fact we find that it is rare for students to go on to learn these
foundations. They usually continue as they were when they leave the course, as
inefficient practitioners, and unable to reason about their analysis.

We have particular experience of this in our teaching of the General Linear
Model (GLM)
\citep{poline2012general}.\footnote{\url{http://imaging.mrc-cbu.cam.ac.uk/imaging/PrinciplesStatistics}}\footnote{\url{http://matthew-brett.github.io/teaching/glm_intro.html}\label{glm_intro}}
The GLM is a widely-used term in neuroimaging, to refer to the generalization
of multiple regression to analysis of variance (ANOVA) and other standard
group comparisons.  Statisticians generally prefer the term ``linear
regression model'' or, simply, ``linear model.''

Functional imaging packages present the linear model as a matrix formulation
of multiple regression.  Many students have not taken linear algebra, or if
they have, do not remember it well.  The traditional teaching approach is to
show how the matrix formulation relates to simple and multiple regression over
a few slides in an hour-long talk, and then move on to more advanced topics.
Although the students may have the impression that they have understood, we
have found that most of them are unable to answer simple questions about the
matrix formulation after such a talk, and that they rarely go on to improve
their understanding in their later education.

% we also believe that it is relatively easy to pick up the big picture once
% you have the foundation

In contrast to the traditional approach, we emphasize:

\begin{itemize}

\item
    Efficient computational practice, to decrease confusion, reduce error, and
        increase collaboration.

\item
    Teaching on the fundamental mathematical and statistical ideas of the
        analysis, such as the GLM, using computational tools to build,
        illustrate and explain.

\end{itemize}

In the course we describe here, we devoted our teaching and assessment time to
building these foundations, on which the students could build their later
analysis and understanding.

We were inspired by the famous epithet of Richard Feynman, found written on
his blackboard after his death: ``What I cannot create, I do not
understand.''\footnote{\url{http://archives.caltech.edu/pictures/1.10-29.jpg}}
We first taught the students to build code efficiently, and then we taught
them how the methods worked, by building them from code.  Our aim was that our
students should be able to implement their own simple versions of the major
algorithms they were using.

\section{Material and Methods}\label{methods}

During the fall semester of 2015, we taught \emph{Reproducible and Collaborative
Statistical Data Science},\footnote{\url{http://www.jarrodmillman.com/stat159-fall2015/}}
a project-based course offered through the Department of Statistics at UC Berkeley.
The course was open to upper-level undergraduates (as STAT 159) as well as
first-year graduate students (as STAT 259).

The course entry requirements were Berkeley courses STAT 133 (Concepts in
Computing with Data), STAT 134 (Concepts of Probability), and STAT 135
(Concepts of Statistics).  Together these courses provide basic
undergraduate-level familiarity with probability, statistics, and statistical
computing in R.
Many students were from statistics and/or computer science; other majors
included cognitive science, psychology, and architecture.
During the 15-week long semester, we gave three hours of class
per week in two 90-minute sessions, plus two hours of labwork.
Students were expected to work at least eight hours per week outside class.
Project reports were due two weeks after the last class.

\subsection{Course overview}

Here is the Berkeley course catalog entry for our course:

\begin{quote}
A project-based introduction to statistical data analysis. Through case
studies, computer laboratories, and a term project, students learn
practical techniques and tools for producing statistically sound and
appropriate, reproducible, and verifiable computational answers to
scientific questions. Course emphasizes version control, testing,
process automation, code review, and collaborative programming.
Software tools include Bash, Git, Python, and \LaTeX.
\end{quote}

Our objective for the course was for students to be
(a) proficient at the Unix command line,
(b) fluent in using version control with Git,
(c) able to write documents in Markdown and \LaTeX,
(d) familiar with scientific computing in Python,
(e) able to understand the computational and statistical issues involved with
reproducibility, and
(f) familiar with computational issues in modern statistical data
analysis through hands-on analysis of neuroimaging data.

In particular, we wanted to show our students how to apply these methods to
investigate a real scientific question, that they had chosen themselves.

\subsubsection{Tools and process}

Our approach was to teach the students efficient reproducible practice with
the standard tools that experts use for this purpose
\citep{millman2014developing}. Our motivating assumptions were twofold.
First, our own experience has taught us that the power of these tools only
becomes clear when you use them to do substantial work in collaboration with
your peers.  Second, we have found that teaching ``easy'' tools that need less
initial learning has the paradoxical effect of making it harder for learners
to move on to the more powerful and efficient tools that they will need for
their daily work.

We applied this approach in our choice of general computing tools and
neuroimaging analysis software.   For example, rather than teaching the user
interface of a specific neuroimaging analysis package, we taught scientific
coding with Python and its associated scientific libraries
\citep{millman2011python, perez2011python}, and then showed the students how to
perform standard statistical procedures on imaging data, using these tools
\citep{millman2007analysis}.

\blockpar{Unix command line.}
The Unix environment is the computational equivalent of the scientists'
workbench \citep{preeyanon2014reproducible}.  The Unix command line, and the
Bash shell in particular, provides mature, well-documented tools for building
and executing sequences of commands that are readable, repeatable, and can be
stored in text files as scripts for later execution.  Quoting
\cite{wilson2014best}---the Bash shell makes it easier to ``make the computer
repeat tasks'' and ``save recent commands in a file for re-use.''

The GUI interface of operating systems such as Windows and macOS can obscure
the tree structure of the filesystem, and this in turn makes it harder for GUI
users to think about the organization of data in hierarchy of directories and
files.  The command line tools make the file system hierarchy explicit, and so
make it easier to reason about data organization.

Linux and macOS are native Unix platforms and provide Unix / Bash command line
tools from a standard install.  Windows 10 provides the Unix / Bash tools as
an optional operating system install.

\blockpar{Version control with Git.}
Version control is a fundamental tool for organizing and storing code and
other products of data analysis.
Distributed version control allows many people to work in parallel on the
same set of files.
The tools associated with distributed version control make it easier for
collaborators to review each other's work and suggest changes.

Git is the distributed version control system that has become the standard in
open source scientific computing. It is widely used in industry.
There are automated installers for all major operating systems.
Web platforms such as GitHub, Bitbucket, and GitLab provide web interfaces
to version control to simplify procedures such as code review, automated
testing and issue tracking (see below).

\blockpar{Scientific computing with Python.}
Python is a general purpose programming language,
popular for teaching and prevalent in industry and academia.
In science, it has particular strength in
astronomy and genetics, and data science.
Its impact in scientific computing rests on a stack of popular scientific
libraries including NumPy (computing with arrays), SciPy (scientific
algorithms including optimization and image processing), Matplotlib (2D
plotting), Scikit-Learn (machine learning), and Pandas (data science).  The
Nibabel library \footnote{\url{http://nipy.org/nibabel}\label{nibabel}} loads
file in standard brain image formats into memory as arrays for manipulations
and display by the other packages.

\blockpar{Pull requests and peer review.}
Regular peer review is one of the most important ways of continuing to learn
to be an effective author of correct code and data analysis.  Git and its
various web platforms provide a powerful tool for peer review.

``Pull requests'' are a Git interface feature pioneered by GitHub, that
presents proposed changes to the shared repository in a convenient web
interface. This allows collaborators to comment on the changes, ask questions,
and suggest improvements.  The discussion on this interface forms a record of
decisions, and a justification for each accepted set of changes.

GitHub, like other Git hosting platforms, provides an interface for creating
``Issues.''  These can be reports of errors in the code or analysis that need
to be tracked, larger scale suggestions for changes, or items in a To Do list.

\blockpar{Functions and unit tests.}
Functions are reusable blocks of code used to perform a specific task.
They usually manipulate some input argument(s) and return some output
result(s).
There are several advantages to organizing your code into functions rather
than simply writing stream of consciousness scripts.
First, functions allow you to ``hide'' the details of a discrete bit of
code from view.
This can make it easier to see the higher-level structure of your analysis.
Second, it encourages you to think of your program as a collection of sub-steps.
Third, it allows you to reuse code instead of rewriting or copying and editing
it.
Fourth, it enables you to keep your variable namespace clean.
Finally, it also allows you to test small parts of your program in isolation
from the rest.

Testing is an essential discipline to give some assurance to the authors and
other users that the code
continues to work as expected. The tests should also document how the code is
expected to be used.
Inexperienced coders typically greatly underestimate how often they make
errors; expecting, finding and fixing errors is an essential part of being
able to learn from continued practice.  We taught students to test their code
thoroughly.  There are tools that can measure how what proportion of the lines
of code have covered by the tests---this is ``test coverage.''

% unit tests doctests

\blockpar{Continuous integration.}
Tests are not useful if they are not run, or if the authors do not see the
results.  Modern code quality control includes ``continuous integration
testing,'' a practice that guarantees that the tests are run frequently and
automatically, and the results are reported back to the coding team.

Fortunately, this practice has become so ubiquitous in open-source, that there
are large free services such as Travis CI
\footnote{\url{https://travis-ci.org}} that integrate with code hosting sites
like GitHub. We set up each project with a configuration file for Travis CI
such that all their tests would be run and reported on remote servers, each
time they changed their code on the GitHub hosting site.  We added automated
code coverage testing; each change to the code on GitHub resulted in a measure
of the proportion of all code lines covered by tests.

\blockpar{Document processing and markup languages.}
Separating the content from the format of a document is important for reducing
visual clutter as you write and edit documents.
A markup language allows for generation of content in plain text, while also
maintaining a set of special syntax to annotate the content with structural
information.
In this way, the plain-text or ``source'' files are human-readable and easy to
use with version control.
To produce the final document, the source files must undergo a build or render
step where the markup syntax (and any associated style files) are passed to a
tool that knows how to interpret and render the content.

\fixme{Document generation with \LaTeX and pandoc.}

\blockpar{Reproducible workflows with Make.}
The venerable Make system was originally written to automate the process
of compiling and linking programs.
However, when working in the Unix command line environment, you are constantly
generating files by performing a sequence of commands perhaps depending on
other files.
%While exploring running these commands by hand is common.
Makefiles are machine-readable text files that associate labels for tasks with
the recipe for that task.

For example a Makefile could define a label \texttt{get\_data} with its
associated recipe, like this:

\begin{verbatim}
    get_data:
        # Fetch the data archive from Amazon Web Services
        wget -N http://openfmri.s3.amazonaws.com/tarballs/ds005_raw.tgz
        # Unpack the data into the current directory
        tar zxvf ds005_raw.tgz
\end{verbatim}

Like functions in code, using these labels can make it easier to see the
structure of a set of tasks.  For example, the whole analysis might be
expressed as a series of labels supplied to Make:

\begin{verbatim}
    make get_data
    make validate_data
    make analysis
    make figures
    make report
\end{verbatim}

\subsubsection{Lectures, labs, notes, homeworks, readings, and quizzes}

The weekly lectures and labs prepared students---through a
series of examples and guided exercises---with the basic skills and
background needed for the group project.
We demonstrated basic methods and tools during lecture and lab,
and expected students to do additional research and reading as they worked on
the group project.
For example, we only gave one or two lectures on each of Unix, Git, Python,
Testing, and NumPy.
Each weekly lab would focus on the software stack from previous lectures;
after the Git lecture, we began to use Git and GitHub to fetch and submit all
exercises;
after we taught on testing, we continually reinforced testing in the labs.
Course work outside labs also made heavy use of the toolstack, including each
new tool as it was introduced in lectures.

% first lecture topic
For the first three lectures, we introduced students to the Unix environment
and started teaching them version control using Git.
The fourth lecture was a high-level introduction to neuroimaging and
functional magnetic resonance imaging (or FMRI).
We then introduced students to scientific computing with Python.
Students were referred to our course notes on
Bash,\footnote{\url{http://www.jarrodmillman.com/rcsds/standard/bash.html}}
Git,\footnote{\url{http://www.jarrodmillman.com/rcsds/standard/git-intro.html}}
Make,\footnote{\url{https://www.youtube.com/watch?v=-Cp3jBBHQBE}}
%\footnote{\url{https://github.com/berkeley-scf/tutorial-make-workflows}}
Python,\footnote{\url{http://www.jarrodmillman.com/rcsds/standard/python.html}}
scientific Python packages,\footnote{\url{http://www.scipy-lectures.org/}}
and \LaTeX.\footnote{\url{https://www.youtube.com/watch?v=8khoelwmMwo}}
%\footnote{\url{http://github.com/berkeley-scf/tutorial-latex-intro}}

% second lecture topic
By the ninth lecture, we started focusing on analyzing FMRI data with Python.
The data analysis lectures continued to develop the students skills with
Unix and scientific Python software stack through practical problems,
which arose as we showed them how to analyze FMRI data.

During the first part of the semester, we assigned two homeworks.
Students had two weeks to work on each homework.
Homeworks were assigned and submitted using private GitHub repositories.
Typically, assignments were given as a collection of function templates with
missing implementations.
The functions were documented according to the NumPy documentation
standard\footnote{\url{https://github.com/numpy/numpy/blob/master/doc/HOWTO_DOCUMENT.rst.txt}}
and included unit tests.
See the code listing in Figure~\ref{fig:dna} for an example from the first homework.
Students implemented the functions according to the documentation
and ran the unit tests to check that their code at least
returned the correct results for the provided tests.
Assignments were graded using unit tests, which were not provided to
the students before they submitted their work.
The homework reinforced the material we taught during the
beginning of the course and mainly focused on scientific programming
in Python.

%GitHub, doctest/unit tests, functions, basic Python and scientific Python...

\begin{figure}
\centering
\input{dna.tex}
\caption{Above we list the first few lines of \texttt{dna.py}.
The assignment was to edit this file by implementing the functions.
Including \texttt{is\_dna}, there were 9 functions
to implement in \texttt{dna.py}.
The first homework consisted of this Python file as well as two others.
It also included a \texttt{README} file, which included instructions
such as:}\label{fig:dna}
\begin{quotation}
The functions you are to implement are already defined in the \texttt{dna.py} and
\texttt{cipher.py} files respectively. The string that immediately follows the function
definition is called the \emph{docstring}. (The function definition is the line
starting with \texttt{def}). In this case, you are to use the description given in the
docstring as a guide for how to implement the function. At the end of each
docstring, there is an \texttt{Examples} section that contains lines that start with
\verb|>>>|. These are called \emph{doctests} as they are \emph{tests} for the code that are
contained in the docstring. If you run the doctests on the assignment before
doing any work, you'll notice that all of the tests fail. That's because none
of the functions have been implemented yet! Once you have implemented a
function, you can run the doctests again to make sure that function passes all
of its doctests.
\end{quotation}
\end{figure}

Over the course of the semester, seven readings were assigned on a roughly
bi-weekly basis.
The readings consisted of articles that emphasized the core concepts
of the class, either with respect to scientific computing, or neuroimaging.
For the ``assignment'' portion of the reading, students were asked to compose
a two-paragraph write-up that both summarized the main points of the article,
and commented on it from their personal perspective.

The labs and quizzes emphasized hands-on experience with the computing tools
and process associated with reproducible and collaborative computing (e.g.,
version control, \LaTeX, etc.).
Multiple choice quizzes were held at the beginning of lab sessions, and
emphasized the computing aspects of the course material as
opposed to the statistical and neuroimaging components covered in the lectures.
The remainder of the lab was devoted to providing hands-on experience via
collaborative work on breakout exercises.
These breakout exercises were formulated as small ``projects'' to be worked on
in groups of 3 to 4 students.
Prior to the breakout session, a small review of material relevant for the 
exercises was presented, often in the form of an interactive Jupyter notebook.

We also used the labs to help the students practice critical code review.
For example, we had the students form small groups and review one another's
solutions to the second homework.
This involved all the students in each group creating pull requests to a
shared GitHub repository with their solutions.
Then they had to use the GitHub code review model, which we had been practicing
in class, to review three functions in each solution.
We asked them to summarize their findings making sure to address
clarity, brevity, and performance.
We used their summaries to inform a subsequent discussion about code review.
%http://www.jarrodmillman.com/rcsds/labs/lab09.html#exercise-code-review

\subsection{Course project}\label{project}

The course centered around a semester-long group project accounting for 55\%
of the overall course grade.
Students worked in teams on their projects for three months.
%Students formed teams at the start of week 5 and final reports were submitted
%at the end of week 17.
We centered the course on the group project because we believe computational
reproducibility and collaboration are too abstract in the absence of a
significant concrete project.
If a project lasts just a few weeks, it is easy to remember all the steps
without carefully recording and explaining them.
If you have a toy dataset and a small handful of tiny functions, you can just
throw them all in a directory and post them online.  As the datasets get
larger and the analysis more complex, it becomes progressively harder to keep
track of the analysis; this situation is typical of the analysis of real data.

%\begin{quote}
%\begin{flushleft}
%Form teams \dotfill Week 5\\
%Project proposal \dotfill Week 6\\
%Progress presentation \& draft report \dotfill Week 12\\
%Project presentation \dotfill Week 15\\
%Final report \dotfill Week 17\\
%\end{flushleft}
%\end{quote}

Early in the semester (week~5) groups of three to five students formed teams.
To ensure every aspect of the project work was done in a computationally
reproducible and collaborative way, we immediately created a publicly visible
project repository for each team.
They then had a week to propose what they would do for their projects.
We required them to use a \LaTeX template for their project proposals
and to add the source file(s) to their project repository (see Figure~\ref{fig:repo}).
The project proposals involved
(a) identifying a published FMRI paper and the accompanying data,
(b) explaining the basic idea of the paper in a paragraph,
(c) confirming that they could download the data and that what they
downloaded passed basic sanity checks (e.g., correct number of subjects), and
(d) explaining what approach they intended to take for exploring
the data and paper.
All teams chose a paper and accompanying dataset from
OpenFMRI,\footnote{\url{https://www.openfmri.org/}} a publicly-available
depository of MRI and EEG datasets
\citep{poldrack2013toward,poldrack2015openfmri}.
%Each team chose a published neuroimaging study to investigate and
%decided what they meant by \emph{investigate} in consultation with us.


\begin{figure}
\centering
\input{tree.tex}
\caption{General caption ...}
\label{fig:repo}
\end{figure}

We read and discussed their project proposals and then meet with each
team to help them refine their initial ideas.
Most teams' proposals were initially too ambitious, so our feedback
was mostly around helping the students come up with more manageable
goals.
\fixme{just the beginning of a collaborative process...
This continued throughout the semester and was quite involved.
Mirrors how we work with collaborators.}

The scope of the projects were defined iteratively: each team submitted a 
proposal a third of the way through the semester, with the instructors providing
prompt feedback to help clarify the motivation and goals of each project.
This feedback process continued throughout the semester, with students 
submitting drafts according to milestones defined within the project
timeline.

Feedback was not only provided to the students from the instructors: the
project evaluations included peer-review, where each research team was
responsible for cloning another team's project, running the tests and analyses,
and evaluating the resulting technical document.

The peer-review process proved particularly valuable, as the students 
benefited greatly from the exposure to the coding techniques and 
organizational principles of their peers.
% an advantage of having all students working on their own work

We required students to use GitHub's pull request (PR) mechanism to review all
project work.
While we allowed students to decide when to merge PRs, we recommended that
all PRs should have at least three participants, test coverage should not decrease,
and tests should pass before being merged.
We covered this workflow in depth during lectures and in lab,
and made clear that the project grade would be based on
(a) the final report,
(b) the analysis code,
(c) whether we could run their analysis code to reproduce all
their results and figures, and
(d) how effectively the team collaborated using the techniques
taught in the course (e.g., pull requests, code review, testing).
%https://github.com/berkeley-stat159/project-template

Students also used the GitHub Issues interface to record project discussions
and actions taken by the group members.
Moreover, students were instructed to create a PR
or issue with code and text and use the GitHub interface to ask one or more
of us to respond, if they had any questions about their projects.
Periodically, we would review their work and open issues.
We told the students that we would use the PR and Issue discussions
as evidence for their contributions to the project, and as data for their
final grades.

Rather than having students commit raw data to their project repository, we had
them commit code to download and validate (using stored file checksums)
all raw data used in their project.

Most code was written as a collection of functions with tests with short
scripts that called these functions to perform the project data analysis.
\fixme{code review, continuous integration, coverage reporting}
\fixme{What was the test coverage of the individual projects?}
We expected students to keep test coverage high, typically above 90\%.

Slides were written in Markdown.
These plain text files were committed to the project repository.
They used Pandoc---a command line document processor---to generate
PDFs that could be used as presentation
slides.\footnote{\url{http://pandoc.org/MANUAL.html\#producing-slide-shows-with-pandoc}}
Reports were written in \LaTeX.
They had to commit the final PDF of their reports as well as
provide instructions for generating the PDF from the source
\LaTeX files.

Each project was required to put a \texttt{README} file at the top-level
of their repository to explain how to install, configure,
and run all aspects of their project.
We told the students that we would only look at this \texttt{README}
when determining whether we could reproduce their work.
Each \texttt{README} clearly stated  how to operate the
components of the project and in what order to do so.
In particular, they were required to include a section with
an enumerated list of task to carry out, such as:
\begin{verbatim}
    make data - downloads the data 
    make validate - ensure the data is not corrupted
    make eda - generate figures from exploratory analysis
    make analysis - generate figures and results
    make report - build final report
\end{verbatim}

The \texttt{README} also contained other pertinent information:
for example, if the \texttt{make data} step downloads 5 Gb of data, it should
warn users of this so they make sure they have enough disk space and are on a
stable internet connection before trying to do so.

\fixme{To ensure that all the projects would work.
We offered to proofread everyone's reports and
test that we could reproduce their reports.
As always, we used the GitHub system to record all feedback.
We also required that each group prepare a slide presentation
discussing where things were and there plan for finalizing
their project two weeks before they were due.
Finally, we had each team reproduce another team's project.}

\fixme{More details about 'each team reproduce' ... ?}

\subsection{Neuroimaging data analysis}\label{analysis}

When teaching neuroimaging data analysis, our aim was for students to
(a) understand the basic concepts in neuroimaging,
and how they relate to the wider world of statistics, engineering, and computer science;
(b) be comfortable working with neuroimaging data and code, so they can write
their own basic algorithms, and understand other people's code;
(c) continue to use the computational techniques we had taught them to improve
efficiency and help their understanding.

For this course we concentrated on the statistical analysis of FMRI data.  We
scheduled the teaching to give the students a sound background in standard
neuroimaging statistical analysis with a linear model (GLM).

All the following teaching used simple Python code to show them how the
mathematics works in practice, and to give them sample code that they could
edit and run for themselves -- see below for some examples.  We specifically
avoided using off-the-shelf imaging analysis software packages, and encouraged
the students to build their own analyses given the building blocks we had
given them.  We interleaved teaching with short group exercises where students
took the code from class and extended it to solve a new problem.

We covered the following topics:

\begin{itemize}

\item
    The idea of images as visual displays of values in arrays.
\item
    The standard neuroimaging NIfTI image format as a simple image container,
        with metadata (such as shape) and image pixel / voxel data stored as
        a sequence of numbers.  We used the Nibabel Python package
        \cref{nibabel} to load NIfTI data as three-dimensional arrays.
\item
    Four dimensional (4D) images as sequential time-series of 3D image
        volumes.  Slicing arrays to get individual volumes from the 4D
        time-series.  Extracting single-voxel time-courses from 4D images.
\item
    Building regressors for the statistical model.  We introduced the idea of
        the neuronal time-course model as the hypothesized change in neuronal
        activity caused by the task.  For a block design, this model becomes a
        box-car, with all rest periods having identical low activity, and all
        activity periods having identical high activity.  The FMRI acquisition
        measures something like blood flow, rather than neuronal activity, but
        blood flow changes slowly in response to changes in neuronal activity.
        We introduced the linear-time-invariant assumption in predicting the
        hemodynamic activity from the hypothesized neuronal activity, and then
        the hemodynamic response function as the invariant response.  Finally
        we demonstrated the mechanism of convolution as a numerical method to
        express the effect of the hemodynamic response function acting in a
        linear-time-invariant way, and the caveats to these assumptions.  We
        now had hemodynamic regressors to fit to our voxel time-course data.
\item
    Correlation as a simple test of linear association between the hemodynamic
        regressor and voxel signal.  Calculating correlation with a single
        regressor for every brain voxel and the idea of a statistical
        parametric map.
\item
    The GLM as a matrix formulation of multiple regression.  We started with
        simple (single-regressor) regression as another measure of linear
        association.  We expressed simple regression as a linear model and
        showed how the model can be expressed as the addition of vectors and
        vector / scalar multiplication.  This leads to the matrix formulation
        of simple regression, and thence to multiple regression.  We introduce
        dummy indicator variables to express group membership and show how
        these relate to group means.  We showed with code how this mathematics
        can express statistical methods that they already know, such as
        regression, $t$-tests, and ANOVA.
\item
    High-resolution sampling of regressors.  The simple cases that we had
        covered above assumed events or blocks that started at the same time
        as the scanner started collecting a new brain volume.  This is not the
        case in general.  To deal with events that do not start at volume
        onsets, we need to generate a hemodynamic time-course at higher
        time resolution than the spacing of the scan onsets, and then sample
        this regressor (using linear interpolation) at the scan onset times.
\item
    Parametric modulation of event regressors.  Some of the OpenFMRI datasets
        that the students had chosen used regressors to model parametric
        module of events.  There is one regressor to model the hemodynamic
        effect of a particular event type on average, and another to capture
        the variation of the hemodynamic event activity as a linear function
        of some characteristic of that event, such as duration or intensity.
\item
    Spatial smoothing with a Gaussian kernel; smoothing as a form of
    convolution.

\item
    The idea of voxel and millimeter coordinates in brain images, and the
    image affine as a mapping between them.  The students needed this in order
    to relate the coordinates reported in their papers to voxels in their own
    analyses.

See \cref{glm_intro} for an example lesson showing our use of Python code to
illustrate the mathematical and statistical ideas.

% An exercise as illustration.

%%% Jarrod - can you use your LaTeX foo to put the top of
%%% assignments-2015/diagnostics/diagnostics.md here as a figure?

% Kind-of flipped.
% Centered on the project.

\end{itemize}

\section{Results}\label{results}

There were a total of eleven different research teams composed of three
to five students, each responsible for completing a final project of their
own design using datasets available through the OpenFMRI organization.

The evaluation of the final projects was based on criteria that emphasized the
underlying principles of reproducibility, collaboration, and technical
quality.
See Table~\ref{tab:rubric} for the final project grading rubric.
From the perspective of reproducibility, projects were evaluated on whether the
presented results could be generated from their repositories according to the
documentation they provided.
The code tests were also evaluated with respect to code coverage
and thoroughness.
The collaborative aspects of the project were evaluated using the information
from the project history provided by version control, including
individual contributions and reviewing pull requests.
Finally, the technical quality of the project was evaluated both in terms of
the clarity of the final report and with respect to how well the proposed
goals of the study were met by the final results of the analysis.

\begin{table}
\centering
\begin{tiny}
\input{rubric}
\caption{Project grading rubric.
An ``A'' was roughly two or more check pluses and no check minuses.}
\label{tab:rubric}
\end{tiny}
\end{table}

There was a large disparity in the backgrounds of the students that constituted 
each research team, and this was reflected by a range of scopes for the
final projects.
The primary focus of the majority of projects was reproducing results
of published research from the OpenFMRI datasets using the neuroimaging 
analysis methods introduced in the course.
For example, several teams chose to focus on the open dataset provided by a
study entitled \textit{The neural basis of loss aversion in decision-making
under risk} \citep{tom2007neural}.
The research teams implemented analyses based on the linear model (GLM) as
well as extending the analysis to include other regression techniques, comparing
their results to those of the published study.
Projects of this nature accounted for about 8 of the 11 research teams, and
on the whole were largely successful in demonstrating a practical understanding
of the relevant statistical techniques.
However, several teams expressed interest in using the open data sets to 
perform analyses beyond replicating the published results.
The investigation of modern machine learning techniques was of particular 
interest to several groups that included students with a stronger computer
science background.
These projects utilized supervised learning techniques in an attempt to 
develop predictive models based on input FMRI images.
The ability of students to apply these concepts (which were largely external
to the course) within the given reproducible and collaborative 
framework was impressive and, according to feedback from the students,
motivating and rewarding for them as well.
In all cases, we were pleased by the quality of the organization and emphasis
on reproducibility of the final projects, especially given the fact that
most of the students had no practical experience with the tools (Python, Git,
etc.) prior to this course.
Most teams demonstrated a solid understanding of the neuroimaging methods introduced
in the course by implementing analyses to replicate published results; and 
several went beyond this to expand their investigations to novel applications
of the open data.

\fixme{I feel the need for a bit more detail about what they implemented in
their own code?  Did they mostly use custom code for the GLM?  For PCA?}

\section{Discussion}\label{discussion}

Most neuroimaging researchers agree that computational reproducibility is
desirable, but rare.  How should we adapt our teaching to make reproducibility
more common?

We believe the most common answer to this question, is that we should train
neuroimaging and other computational researchers as we do now, but add short
courses or bootcamps that teach a subset of the tools we taught here but without
the substantial practice.
That is, reproducibility is an addition on top of current training.

We believe this approach is doomed to failure, because the students are
building on an insecure foundation.  Once you have learned to work in an
informal and undisciplined way, it is difficult to switch to a process that
initially demands much more effort and thought.  Rigor and clarity are hard to
retrofit.
To quote the American chemist Frank Westheimer:
``A couple of months in the laboratory can frequently save a couple of hours
in the library.''

For these reasons, our course took the opposite to the common approach. We
started with the tools and process for working with numerical data, and for
building their own analyses. We used this framework as a foundation on which
to build their understanding of the underlying ideas.  As we taught these
tools, we integrated them into their exercises, and made it clear how they
related to their work on the project.

Our claim is that this made our teaching and our students much more efficient.
The secure foundation made it easier for them to work with us and with each
other. As they started their project work, early in the course, they could
already see the value of these tools for clarity and collaboration. Our
students graduated from the course with significant experience of using the
same tools that experts use for sound and reproducible research.

\subsection{Did we really teach neuroimaging analysis?}

By design, our course covered tools and process as well as neuroimaging.  We
used neuroimaging as the example scientific application of these tools.  Can
we claim to have taught a substantial amount of neuroimaging in this class?

Class content specific to neuroimaging was a guest lecture in class 4 (of 25),
and teaching on the general linear model and principal component analysis from
classes 9 through 15.  We only attempted to cover the standard statistical
techniques needed for a basic analysis of a single run of FMRI data.  This is
a much narrower range than a standard neuroimaging course.  However, we argue
that we covered these topics in much greater depth that is typical for a
neuroimaging course, and that this formed the foundation for final projects
that were substantial and well-grounded.

% note to mb: students discovered problems in extending what we showed them,
% but were able to notice the difference and propose solutions
As we argued in the introduction, typical imaging courses do not attempt to
teach the fundamental ideas of linear models, but assume this understanding
and move onto imaging specifics.  This assumption is almost entirely false for
neuroscience and psychology students, and mostly false for our own
students, most of whom already had some training from an undergraduate
statistics major. As a result of this incorrect assumption, it is rare for
students of neuroimaging to understand the statistics they are using.  We
taught both the linear model and principal components analysis from the first
principles of vector algebra, using the tools they had just learned to build a
simple analysis from basic components.  As a result, when the students got to
their projects, they had the tools they needed to build their own analysis
code for the neuroimaging data, both demonstrating and advancing their own
understanding.

We also note the difficulty of the task that we gave the students, and the
extent of their success.  We made clear that their project was an open-ended
exploration of an FMRI dataset and paper.  Very few students had any
experience or knowledge of FMRI before the course. The only guidance we gave
was that they should prefer well-curated datasets from the OpenFMRI
depository.  In order to design and implement their project, they had to
understand at least one published FMRI paper in some depth, with little
assistance from their instructors.  We gave no example projects for them to
review, or templates for them to follow.  As we have already described in the
results, the submitted projects were all substantial efforts to reproduce and
/ or extend the published results, and all included analysis code that they
had written themselves.

We did not teach the full range of neuroimaging analysis.  For example, we did
not cover pre-processing of the data, random effects analysis, or inference
with multiple comparisons.   Our claim is that what we did teach was a sound
and sufficient foundation from which they could write code to implement their
own analyses.  We argue that this is a level of competence that few
neuroimagers achieve even after considerable practical experience.

\subsection{Did we teach the right tools?}

We do not believe that the individual tools we chose were controversial. We
consciously taught the tools that we use ourselves, and that we would teach to
students working with us.

We could have used R instead of Python, but Python is a better language for
teaching, and has seen more development for neuroimaging and machine learning.

A more interesting question is whether we went too far in forcing the students
to use expert tools.  For example, we required them to write their their
report in \LaTeX, and their presentation slides and analysis description in
plain text with Markdown markup.  We asked them to do all project interaction
using GitHub tools such as the issue tracker and pull requests.

We set and marked code exercises with Git version control and text files,
rather than interactive alternatives, such as Jupyter Notebooks.

Of course some students complained that they would rather use Facebook for
code discussions and Powerpoint for their presentation slides.
Should we have allowed them to do this?  We believe not.  Our own experience
of using the tools we taught is that their true power only becomes apparent
when you learn to use them for all the tasks of analysis, including generating
reports and presentations.  Mixing ``easy'' but heavy tools like Powerpoint
and Facebook with ``simple'' and light tools like text editors and Markdown
causes us to lose concentration as we switch modes from heavy to light and
back again. It is easy to be put off by the difficulty of getting used to the
light tools, and therefore fail to notice the clarity of thought and
transparency of process that they quickly bring.  Successfully switching from
heavy to light is a process that requires patience and support; it is best
done in the context of a class where there are examples of use from coursework
and support from experienced instructors who use these tools in their daily
work.

% Anecdote: students (5+) coming up on campus to remark how much they liked the
% tools and process. they say the are using them for new work.

\subsection{Do students need to be trained in programming?}

One common justification for not training students in good coding practice, is
the assertion that scientists do not need to be programmers.  Scientists do
have to use, read and write code, so a more realistic version of this
statement would be that scientists do not need to be good programmers.
Unfortunately bad programmers are inefficient and prone to error; they are
less likely to detect errors, and improve slowly over time, if at all.  We
should invest our work in education to help the student work efficiently and
continue learning over the whole of their career.

\subsection{What background do students need?}

The requirements for our course were previous classes giving some familiarity
with probability, statistics, and the use of the R programming language.

We would be happy to relax the requirement for probability.  We did not refer
to the ideas of probability in any depth during the course.  Authors JBP and
MB have taught similar material to neuroscience and psychology students who
lack training in probability; the pace of teaching and level of understanding
were similar across the statistics students in this course and the other
students we have taught.

Psychology and neuroscience students do have some statistical training.  We
believe this background is necessary for us to be able to as start quickly as
we did in the analysis of linear regression.

We would also keep the requirement for some programming experience.  Our
course went quickly through basic programming ideas such as for loops,
conditionals, and functions.  We could not have done this for students without
any experience of programming.  In our psychology / neuroscience courses that
used similar material, we required some experience of programming in a
language such as Python, R or Matlab.  It is possible that a brief
introduction would be enough to fulfill this requirement, such as a bootcamp
or multi-day intensive course.

\subsection{Where would such a course fit in a curriculum?}

Our course would not fully qualify a student for independent research in
neuroimaging.  As we discuss above, we did not cover important aspects of
imaging, including spatial and temporal pre-processing of data, random effects
or control of false positives in the presence of multiple comparisons.  Where
should the elements of our course sit in relation to a larger program for
training imagers and other computational scientists?

We think of this course as a foundation.  Students graduating from this course
will be more effective in learning and analysis.  The tools that they have
learned allow the instructor to build analyses from simple blocks, to
show them how algorithms work, and make them simple to extend.  We therefore
suggest that course like ours should be first in a sequence, where the
following courses would continue to use these tools for exposition and for
project work.

A full course on brain imaging might start with an introduction to Python and
data analysis, possibly as a week-long bootcamp at the start of the semester.
Something like our course would follow.  There should be follow-up courses
using the same tools for more advanced topics such as machine-learning,
spatial pre-processing, and analysis of other modalities. We suggest that each
of these courses should have group project work as a large component, in which
the student can continue to practice techniques for efficient reproducible
research.

\subsection{What factors would influence the success of such a course?}

We should note that there were factors in the relative success of this
course that may not apply in other institutions.

Berkeley has as a strong tradition in statistical computing and
reproducibility.  Leo Breiman was a professor in the Berkeley statistics
department, and an early advocate of what we would now call data science.  He
put a heavy emphasis on computational teaching to statisticians.  Sandrine
Dudoit is a professor in the statistics and biostatistics departments, and a
founding core developer of the Bioconductor project devoted to reproducible
genomics research in R.  The then head of the statistics department, Philip
Stark, is one of many Berkeley authors (including KJM) to contribute to the
recent book ``The Practice of Reproducible Research''
\citep{kitzes2017practice}. Outside statistics, several imaging labs in
Berkeley take this issue seriously and transmit this to their students.  If
there had not been such local interest in the problem of reproducibility,
students may have been less convinced of the importance of
working in a reproducible way, especially given the short-term convenience
of less disciplined working process.

% KJM, FP writings, courses?

Students of statistics and other disciplines at Berkeley are well aware of the
importance of Python in scientific computing, and in industry.  Tech firms
recruit aggressively on campus, and Python is a valuable skill in industry.
The cross-discipline introductory course in Fundamentals of Data Science uses
Python. The new Berkeley Institute of Data Science has a strong emphasis
towards Python for scientific computing.

In the same way, a booming tech sector nearby made it more obvious to students
that they would need to learn tools like Git and GitHub, as these are widely
used in the tech industry.  For example, public activity on GitHub is one
factor companies use to identify candidates for well-paid positions as
software engineers.

We required students to use \LaTeX to write their final reports.  This was an
easy sell to statistics students, as \LaTeX is widely used in statistics.
Students outside mathematical fields might be less agreeable; if we were
teaching psychology and neuroscience students, we might choose
another plain text markup language, such as Markdown, with the text files run
through the Pandoc \footnote{\url{http://pandoc.org}} document processor to
write publication quality output.

\section{Conclusion}\label{conclusion}

We conclude with one of the undergraduate student's responses to the question
\emph{``What advice would you give to another student who is considering taking this course?''}:
\begin{quotation}
``[U]nlike most group projects (which last for maybe a few weeks tops or
could conceivably be pulled off by one very dedicated person), this one will
dominate the entire semester. . . . Try to stay organized for the project and
create lots of little goals and checkpoints. You should always be working on
something for the project, whether that's coding, reviewing, writing, etc. Ask
lots of questions and ask them early!''
\end{quotation}

\section*{Conflict of Interest Statement}

The authors declare that the research was conducted in the absence of any
commercial or financial relationships that could be construed as a potential
conflict of interest.

\section*{Author Contributions}

KJM was the lead instructor and was responsible for the syllabus and project timeline;
KJM and MB were responsible for lectures and created homework assignments;
KJM and RB were responsible for labs, readings, quizzes, and grading;
all authors held weekly office hours to assist students with their projects.
KJM and MB wrote the first draft of the manuscript;
all authors wrote sections of the manuscript, contributed to manuscript revision, 
as well as read and approved the submitted version.
