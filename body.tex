\section{Introduction}

\fixme{
traditional approach vs. our approach to teaching neuroimaging analysis;
bottom up (low-level) vs. top down (high-level);
project-based;
interdisciplinary;
computational reproducibility \citep{buckheit1995wavelab} 
}

# * JB: Rationale 1:  Reproducibility in life/science and neuroimaging 
# * JB: Rationale 2:  Difficulty in collaborating on research projects

We begin by explaining the background and rationale (\S~\ref{background}) of our approach to the course.
In material and methods (\S~\ref{methods}), we describe the course format, student audience,
and curriculum.
Student projects are presented in results (\S~\ref{results}).
Finally, we discuss (\S~\ref{discussion}) adapting the course to
different student audiences.

\section{Background and Rationale}\label{background}

Between us, we have many decades experience of teaching neuroimaging analysis.
% Matthew 1997- = 20
% JB 1997ish- 20
Like most other teachers of imaging, we have taught traditional courses with a
mixture of lectures covering the broad general ideas of the analysis, usually
combined with some practical workshops using imaging software.

We also have a great deal of experience of giving practical support for
imaging analysis to graduate students and post docs.

Our experience has gradually brought us to realize that we have three major
problems in the form of traditional teaching.

The first is that our students often leave their education with a superficial
understanding of imaging methods.  They are familiar with the steps, and the
terms, but when we ask them to reason about their analysis, they struggle.

The second problem is that the idealized picture in the lectures bears a
slight resemblance to their everyday work, which involves: using a Unix
computer, often remotely; collating and manipulating data; writing and sharing
scripts in computing languages such as Matlab or the Unix shell-scripting
languages.  Analyses can and do become messy and complicated, leading to
confusion and error.  Most labs expect students to learn how to do this on the
job, and few labs use current computing tools and process to control 
complexity and error.  As a corollary, it is usually difficult for a
researcher to share their analysis with another researcher, even in their own
lab, and it is hard for the PI to check the analysis for errors or invalid
assumptions.

The final problem is that we have made it hard for imagers to collaborate with
other technical fields such as engineering and statistics.  Imaging software
packages are highly specific to brain imaging, and present an interface to
common statistical procedures that our students do not recognize.  Many
students do not have much background in standard engineering tools such as
convolution, the Fourier transform or numerical optimization.  In traditional
teaching, we usually gloss over the details of these fields, which saves us
time, but leaves the students without the understanding or vocabulary to
explain their analysis problem to other scientists in a way that makes it easy
for them to engage.  The result may be that imaging research is more likely to
live in a neuroscience ghetto that is effectively walled off from
collaboration and insight by other scientists.

After years of trying to patch up the problems caused by our own teaching, we
came to the conclusion that the traditional approach is based on a fundamental
error of emphasis.

Functional imaging analysis touches on many theoretical and practical fields,
including signal and image processing, linear algebra, statistics, computing,
coding and data management.  Teaching all these topics in depth would take
longer than most students or institutions have time for.  We need to choose
what we will teach, and what we will leave for the students to pick up later.
We have to decide what is fundamental, and what is peripheral.

The traditional approach takes the top level to be fundamental.  It aims to
give the big picture, leaving the students to fill in the details. The
assumption is that the student will either know or pick up some of the
mathematical and practical foundations later, with the big picture as a
framework to guide them in choosing what to learn.

Our experience is that this assumption is quite incorrect.  It is rare for
students to go on to learn these foundations, and common for researchers to
continue to work with a limited and superficial understanding.  We believe
this has severe consequences for the quality of their work; because we did not
teach them these foundations, they come to believe that they do not need to
know them.  The foundations appear to them to be the preserve of clever
mathematical and computing types; they have no hope of understanding them in
any depth.  They come to treat the analysis as something special and magic,
where the rules are determined by practice, because they cannot hope to reason
about them.

Here we propose an alternative to the traditional approach.  Instead of
concentrating on the big picture, we emphasize the foundations on which the
analysis is built, and their implementation in practice, with code and
computing.  We were inspired by the famous epithet of Richard Feynman, found
written on his blackboard after his death: ``What I cannot create, I do not
understand.''\footnote{\url{http://archives.caltech.edu/pictures/1.10-29.jpg}}
We aim to give the students a sufficient grasp of how the methods work, that
they could implement their own crude versions of the major algorithms.  In
teaching them to work with code, we also teach them standard programming
techniques for reducing complexity and controlling error.  We encourage them
to own their own learning, and explore their data, by devoting a large
proportion of the course and course marks to an open-ended group project that
must be public and completely replicable, down to the figures used in the
final report.

We will concentrate on two examples to illustrate the contrast between the
traditional approach and our own.

\subsection{The General Linear Model}

The General Linear Model (GLM) is a widely-used term in neuroimaging, to refer
to the generalization of multiple regression to analysis of variance (ANOVA)
and other standard group comparisons.  Statisticians generally prefer the term
"linear model" without the ambiguous "General" prefix.

We have been thinking about how to teach the (general) linear model for many
years
\citep{poline2012general}.\footnote{\url{http://imaging.mrc-cbu.cam.ac.uk/imaging/PrinciplesStatistics}}\footnote{\label{glm_intro}
\url{http://matthew-brett.github.io/teaching/glm_intro.html}}

Functional imaging packages present the linear model as a matrix formulation
of multiple regression.  Many students have not taken linear algebra, or if
they have, do not remember it very well.  The traditional teaching approach is
to show how the matrix formulation relates to simple and multiple regression
over a few slides in an hour-long talk, and then move on to more advanced
topics such as temporal filtering or random effects.  Although the students
may have the impression that they have understood, we have found that most of
them are unable to answer simple questions about the matrix formulation after
such a talk, and that they rarely go on to improve their understanding in
their later education.

Our current teaching works through the linear model very slowly. We found that
we needed to do this for the students to see that the matrix formulation of
the linear model is just another way of expressing statistical methods that
they already know, such as regression, $t$-tests and ANOVA.  We start with
simple regression, showing how the regression can be expressed as the addition
of vectors and vector / scalar multiplication.  This leads to the matrix
formulation of simple regression, and thence to multiple regression.  We
introduce dummy indicator variables to express group membership and show how
these relate to group means.  We do all this using simple Python code to show
them how the mathematics works in practice, and give them sample code that
they can edit and run for themselves.\cref{glm_intro}

In contrast, we rely on the students to work through more advanced concepts
such as temporal filtering and random effects.

\subsection{Programming}

Traditional courses do not teach skills for programming; the hope is that the
student will pick up the code that they need while they are working, or by
taking an unrelated general course on programming.  Many lab heads without
programming expertise argue that teaching students to use code is not
necessary, because scientists do not need to be programmers.  Unfortunately,
imaging researchers do need to use, write and share code.  Without training,
our experience suggests that scientists tend to be inefficient programmers;
they take a long time to write code; their code is sloppy and
poorly-structured with many errors.  They improve very slowly with experience.
In practice, imaging researchers {\em are} programmers; but if we do not train
them to learn, they will be incompetent programmers, and they will likely
continue to be incompetent programmers for the rest of their careers.

For this reason, we introduced standard best coding practice from the
beginning of our course, including testing, code review, measuring test
coverage, version control, and continuous automated testing with different
software versions. All final project work used public online repositories for
version control, code review, automated code coverage and testing.  The
projects gave the students an experience of best practice in collaboration
with code and data that they could apply to their future work.

\section{Material and Methods}\label{methods}

During the fall semester of 2015, we co-taught \emph{Reproducible and Collaborative
Statistical Data Science},\footnote{\url{http://www.jarrodmillman.com/stat159-fall2015/}}
a new project-based course offered through the Department of Statistics at UC Berkeley.
The course was open to upper-level undergraduates (as STAT 159) as well as
first-year graduate students (as STAT 259).
Students were required to have a basic undergraduate level familiarity with
probability, statistics, and statistical computing in R.
Many students were from statistics and/or computer science; however, we also
had students from many disciplines including cognitive science, psychology, and
architecture.
During the 15-week long semester, students were provided three hours of class
per week in two 90-minute sessions, plus two hours of labwork.
Students were expected to work at least eight hours per week outside class.
Project reports were due two weeks after the last class.

\subsection{Course overview}

\begin{quote}
A project-based introduction to statistical data analysis. Through case
studies, computer laboratories, and a term project, students learn
practical techniques and tools for producing statistically sound and
appropriate, reproducible, and verifiable computational answers to
scientific questions. Course emphasizes version control, testing,
process automation, code review, and collaborative programming.
Software tools include Bash, Git, Python, and \LaTeX.

\hfill\emph{---from the course catalog description}
\end{quote}

\emph{Reproducible and Collaborative Statistical Data Science} is a
project-based course that introduces students to reproducible and collaborative
statistical research, applied to a real scientific question.
Our objective for the course was for students to
(a) be proficient at the Unix commandline,
(b) be expert at version control with Git,
(c) be able to write documents in Markdown and \LaTeX,
(d) be familiar with scientific computing in Python,
(e) understand the computational and statistical issues involved with reproducibility, and
(f) be familiar with computational issues in modern statistical data
analysis through hands-on analysis of neuroimaging data.

Below we focus on the course project (\S~\ref{project}) and
how we taught neuroimaging data analysis (\S~\ref{analysis}).
First, we briefly describe the class structure and software stack.

\blockpar{Lectures, labs, notes, homeworks, readings, and quizzes.}
The weekly lectures and labs prepared students---through a
series of guided exercises---with the basic skills and
background needed for the group project.
While we demonstrated basic methods and tools during lecture and lab,
students were expected to do additional research and reading in the course
of working on the group project.
For instance, they may have needed to use a specialized analysis
method or a software package not covered in the lectures or labs.

...learn what we need to know, when we need to know it; just-in-time learning 

...homeworks

Over the course of the semester, seven readings were assigned on a roughly
bi-weekly basis.
The readings consisted of articles that emphasized the core concepts
of the class, either with respect to scientific computing, or neuroimaging.
For the ``assignment'' portion of the reading, students were asked to compose
a two-paragraph write-up that both summarized the main points of the article,
and commented on it from their personal perspective.

In principle, the labs and quizzes were designed to emphasize hands-on 
experience with the computing tools and best-practices associated with
reproducible and collaborative computing (e.g. version control, \LaTeX, etc.).
The quizzes were held at the beginning of lab sessions, and heavily emphasized
the computing aspects of the course material as opposed to the statistical and
neuroimaging components covered in the lectures.
Quizzes were multiple choice and assigned and submitted via GitHub.
The remainder of the lab was devoted to providing hands-on experience via
collaborative work on breakout exercises.
These breakout exercises were formulated as small ``projects'' that were to be
attacked by groups of students with 3 to 4 students per team.
Prior to the breakout session, a small review of material relevant for the 
exercises was presented, often in the form of an interactive Jupyter notebook.

\blockpar{The software stack.}
Rather than teaching a specific neuroimaging analysis packages' user interface,
we taught students the scientific Python software stack
\citep{millman2011python, perez2011python} and software development best
software development practices for computational reproducibility \citep{millman2014developing}.
Python is increasingly popular for scientific computing and data science
including for neuroimaging analysis \citep{millman2007analysis}.


\begin{itemize}
\item \textbf{The Unix environment.}
The Unix environment is the computational equivalent of the scientists'
workbench \citep{preeyanon2014reproducible}.
However, many students have limited exposure working with
command line interfaces and text files.
% and the Bash shell, in particular, are 
%Many of the tools and practices
%shell, command line interface, filesystem hierarchy, and

\item \textbf{Version control with Git.}
Git has become the dominant version control system for open source scientific
Python community.

\item \textbf{Pull requests and code review/collaboration.}

\item \textbf{Process automation with Make.}
Make is the workhorse of process (or execution) automation of the Unix environment.

\item \textbf{Scientific computing with Python.}
Python is a general purpose programming language,
popular for teaching and prevalent in industry.
The scientific stack is a collection of add-on packages created by scientists.
For instance, NumPy, SciPy, and matplotlib are the core packages.

\item \textbf{Functions and unit tests.}

\item  \textbf{Continuous integration with GitHub.}

\item \textbf{Document generation with \LaTeX and pandoc.}
Most students are intimately familiar with word processors, but have little experience
working with text files and editors.

\end{itemize}

We gave one or two lectures on Unix, Git, Python, Testing, and NumPy.
Weekly labs focused on this software stack.
Students were referred to our course notes on
Bash,\footnote{\url{http://www.jarrodmillman.com/rcsds/standard/bash.html}}
Git,\footnote{\url{http://www.jarrodmillman.com/rcsds/standard/git-intro.html}}
Make,
Python,\footnote{\url{http://www.jarrodmillman.com/rcsds/standard/python.html}}
scientific Python packages,\footnote{\url{http://www.scipy-lectures.org/}}
and \LaTeX.
Course work made use of the toolstack.
All course work was versioned controlled with Git
and submitted via GitHub.
Finally, the course project extensively exercised this software stack
and practices.

\subsection{Course project}\label{project}

The course focused on a semester-long group project.
Students worked in teams on their projects for three months.
%Students formed teams at the start of week 5 and final reports were submitted
%at the end of week 17.
The reason the project lasted for so long and was such a large portion of the grade (55\%)
is that we believe computational reproducibility and collaboration
are too abstract in the absence of a significant concrete project.
If a project lasts just a few weeks, it is easy to remember all the steps
without carefully tracking them.
If you have a tiny dataset and a small handful of tiny functions, you can just
throw them all in a directory and post them online.
Problems compound as data grows and analysis complexity increases as lines of
code multiply.

%\begin{quote}
%\begin{flushleft}
%Form teams \dotfill Week 5\\
%Project proposal \dotfill Week 6\\
%Progress presentation \& draft report \dotfill Week 12\\
%Project presentation \dotfill Week 15\\
%Final report \dotfill Week 17\\
%\end{flushleft}
%\end{quote}

Early in the semester (week~5) groups of three to five students formed teams
and immediately (week~6) submitted proposals to investigate
a published result using the analysis of neuroimaging data.\footnote{There
was no requirement for students to have a background in neuroscience.
The intention of focusing on neuroimaging data was to provide a concrete problem
domain that exemplifies the types of programming and statistical challenges
present in many modern statistical applications.}
%Groups of three to five students formed teams.
%Each team chose a published neuroimaging study to investigate and
%decided what they meant by \emph{investigate} in consultation with us.
Each team's investigation involved
acquiring, cleaning, and curating data;
formulating scientific questions statistically;
developing appropriate statistical methods
to analyze the data to answer the scientific questions;
implementing those methods in robust, testable, reusable, extensible software;
applying the methods;
visualizing the results;
interpreting the results;
and communicating the results to others.
Every aspect of the project work was done in a computationally reproducible
and collaborative way.


\blockpar{Repository.}
All project artifacts were stored in a repository.
See Figure~\ref{fig:repo} for a skeleton project repository listing.

\begin{quote}\emph{
required to work on your project using GitHub's pull request and code review mechanism;
during lecture, covered the workflow you will be expected to follow;
project grade based on (a) your final report, (b) how well your group work is
reproducible, and (c) how effectively you collaborated using the techniques
taught in the course (e.g., pull requests, code review, testing, etc.)
}\end{quote}

%https://github.com/berkeley-stat159/project-template

If you have questions about your projects, there should be a pull request
or issue with code and text and \texttt{@mention} one or more of the us.

\blockpar{Proposal.}
The project proposal was the first item each team added to their project repository.
...state advantage of having proposal directly attached to record of the
project work
...explain how it worked, timeline, and feedback

\begin{quote}\emph{
Identify a published fMRI paper and the accompanying data.
Explain the basic idea of the paper in a paragraph.
Perform basic sanity check on the data
(e.g., check that you can downloaded and read the files,
confirm that you have the correct number of subjects).
Briefly explain what approach you intend to take for exploring
the data and paper.  If you intend to ``reproduce'' some aspect of the paper,
explain in exactly what sense you mean to do this.  If you are thinking about
validating the data describe the assumptions used in the analysis and indicate
how you might check them.
}\end{quote}

\blockpar{\texttt{README} and \texttt{Makefile}.}
Each project was required to put a \texttt{README} file at the top-level
of their repository to explain how to install, configure,
and run all aspects of their project.
Each \texttt{README} clearly stated  how to operate the
components of the project and in what order to do so.
In particular, they were required to include a section with
an enumerated list of task to carry out, like:
\begin{verbatim}
    make data - downloads the data 
    make validate - ensure the data is not corrupted
    make eda - generate figures from exploratory analysis
    make analysis - generate figures and results
    make report - build final report
\end{verbatim}

The \texttt{README} also contained other pertinent information:
for example, if the \texttt{make data} step downloads 5 Gb of data, it should
warn users of this so they make sure they have enough disk space and are on a
stable internet connection before trying to do so.
 
\blockpar{Data.}
All projects used data from OpenfMRI.\footnote{\url{https://www.openfmri.org/}}
OpenfMRI is a publicly-available depository of MRI and EEG datasets
\citep{poldrack2013toward,poldrack2015openfmri}.

# JB: add advocacy bit on open versioned and citable data

Rather than having students commit raw data to their project repository, we had
them commit code to download and validate all raw data used in their project.
For example, their project \texttt{README} might tell us that \texttt{make data}
downloads all the relevant files for their study and \texttt{make validate}
verifies the content of the downloaded files.

\blockpar{Code.}
Most code was written as a collection of functions with tests with short
scripts that called these functions to perform the project data analysis.

All code submissions were added to the project via GitHub pull requests (PRs).
While we allowed students to decide when to merge PRs, we recommended that
all PRs should have at least three participants, coverage shouldn’t decrease,
and tests should pass before being merged.

Pull requests, code review, continuous integration, coverage reporting

\blockpar{Slides.}
Slides were written in Markdown.
These plain text files were committed to the project repository.
For example, if \texttt{progress.md} is the source file for a progress presentation,
then the \texttt{Makefile} might have the following recipe for building a nicely
formatted PDF file \texttt{progress.pdf}:
\begin{verbatim}
progress.pdf: progress.md
        pandoc -t beamer -s progress.md -o progress.pdf
\end{verbatim}

In addition to using beamer,
Pandoc\footnote{\url{http://pandoc.org/MANUAL.html\#producing-slide-shows-with-pandoc}}
can also output using other formats like reveal.js or slidy.

\blockpar{Report.}
We committed the following three files to each project repository:
\begin{itemize}
\item \texttt{paper/report.tex}
\item \texttt{paper/report.bib}
\item \texttt{Makefile}
\end{itemize}

\subsection{Neuroimaging data analysis}\label{analysis}

When teaching neuroimaging data analysis, our aim was for students to
(a) understand the basic concepts in neuroimaging,
and how they relate to the wider world of statistics, engineering, computer science;
(b) be comfortable working with neuroimaging data and code, so they can write
their own basic algorithms, and understand other people's code;
(c) work with code and data in a way that will save them time, help them collaborate,
and continue learning.%this isn't quite parallel

For this course we concentrated on the statistical analysis of functional MRI
data.  We scheduled the teaching to give the students a sound background in
standard neuroimaging statistical analysis with a linear model.  We covered
the following topics:

\begin{itemize}

\item The NIfTI image format as a simple image container.
\item Four dimensional (4D) images; extracting voxel time-courses from 4D
    images.
\item The neuronal time-course model.
\item The hemodynamic time-course model, the hemodynamic response function and
    convolution;
\item Spatial smoothing with Gaussian kernel;
\item Simple regression, vector formulation, matrix formulation; least-squares
    cost-function; solving with the pseudoinverse; extension to multiple
        regression; dummy variables and matrix formulation of analysis of
        variance.
\item High-resolution sampling of regressors.
\item Parametric modulation of regressors.

\end{itemize}

\section{Results}\label{results}

There were a total of eleven different research teams composed of three
to five students, each responsible for completing a final project of their
own design using datasets available through the openfMRI organization.
The scope of the projects were defined iteratively: each team submitted a 
proposal at the midpoint of the semester, with the instructors providing
prompt feedback to help clarify the motivation and goals of each project.
This feedback process continued throughout the semester, with students 
submitting drafts according to various milestones defined within the project
timeline.
Feedback was not only provided to the students from the instructors: the
project evaluations included peer-review, where each research team was
responsible for cloning another team's project, running the tests and analyses,
and evaluating the resulting technical document.
The peer-review process proved particularly valuable, as the students 
benefited greatly from the exposure to the coding techniques and 
organizational principles of their peers.

The evaluation of the final projects was based on criteria that emphasized the
underlying principles of reproducibility, collaboration, and technical
quality.
See Table~\ref{tab:rubric} for the final project grading rubric.
From the perspective of reproducibility, projects were evaluated on whether the
presented results could be generated from their repositories according to the
documentation they provided.
The code tests were also evaluated with respect to code coverage
and thoroughness.
The collaborative aspects of the project were evaluated using the information
from the project history provided by the version control, including
individual contributions and reviewing pull requests.
Finally, the technical quality of the project was evaluated both in terms of
the clarity of the final report and with respect to how well the proposed
goals of the study were met by the final results of the analysis.

There was a large disparity in the backgrounds of the students that constituted 
each research team, and this was reflected by a range of scopes for the
final projects.
The primary focus of the majority of projects was reproducing results
of published research from the openfMRI datasets using the neuroimaging 
analysis methods introduced in the course.
For example, several teams chose to focus on the open dataset provided by a
study entitled \textit{The neural basis of loss aversion in decision-making
under risk}\cite{tom2007neutral}.
The research teams implemented analyses based on the GLM as well extending
the analysis to include other regression techniques, comparing their results
to those of the published study.
Projects of this nature accounted for about 8 of the 11 research teams, and
on the whole were largely successful in demonstrating a practical understanding
of the relevant statistical techniques.
However, several teams expressed interest in using the open data sets to 
perform analyses beyond replicating the published results.
The investigation of modern machine learning techniques was of particular 
interest to several groups that included students with a stronger computer
science background.
These projects utilized supervised learning techniques in an attempt to 
develop predictive models based on input fMRI images.
The ability of students to apply these concepts (which were largely external
to the course) within the given reproducible and collaborative 
framework was impressive and, according to feedback from the students, quite
motivating and rewarding for them as well.
In all cases, the quality of the organization and emphasis on reproducibility
of the final projects was quite satisfactory, especially given the fact that
most of the students had no practical experience with the tools (python, git,
etc.) prior to this course.
Most teams demonstrated an understanding of the neuroimaging methods introduced
in the course by implementing analyses to replicate published results; and 
several went beyond this to expand their investigations to novel applications
of the open data.

%\url{https://raw.githubusercontent.com/jarrodmillman/rcsds/master/notes/rubric.rst}
%\url{http://www.jarrodmillman.com/rcsds/notes/rubric.pdf}

discuss general quality

discuss a couple of example projects
\citep{tom2007neural}

\input{discussion}

\section*{Conflict of Interest Statement}

The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.

\section*{Author Contributions}

KJM was the lead instructor and was responsible for the syllabus and project timeline;
KJM and MB were responsible for lectures and created homework assignments;
KJM and RB were responsible for labs, readings, quizzes, and grading;
all authors held weekly office hours to assist students with their projects.
KJM and MB wrote the first draft of the manuscript;
all authors wrote sections of the manuscript, contributed to manuscript revision, 
as well as read and approved the submitted version.
